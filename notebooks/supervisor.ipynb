{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77649cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable reloading\n",
    "%load_ext autoreload\n",
    "# all the modules should be reloaded before executing the code\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63522bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc67f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Research Agent Subgraph.\"\"\"\n",
    "\n",
    "try:\n",
    "    from .configuration import Configuration\n",
    "    from .states import ConductResearch, ResearchComplete, StatesKeys, SupervisorState\n",
    "    from .utils import get_notes_from_tool_calls, is_token_limit_exceeded\n",
    "except ImportError:\n",
    "    import rootutils\n",
    "\n",
    "    rootutils.setup_root(search_from=str(Path.cwd().parent), indicator=[\".git\", \"pyproject.toml\"], pythonpath=True)\n",
    "    from src.agent.configuration import Configuration\n",
    "    from src.agent.researcher_agent import researcher_subgraph\n",
    "    from src.agent.states import ConductResearch, ResearchComplete, StatesKeys, SupervisorState\n",
    "    from src.agent.utils import get_notes_from_tool_calls, is_token_limit_exceeded\n",
    "    # from src.agent.prompts import PROJECT_RESEARCH_AGENT_PROMPT, SEARCH_INSTRUCTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize a configurable model that we will use throughout the agent\n",
    "configurable_model = init_chat_model(\n",
    "    configurable_fields=(\"model\", \"max_tokens\", \"api_key\"),\n",
    ")\n",
    "RESEARCH_SYSTEM_PROMPT = \"\"\"You are a research assistant conducting deep research on the user's input topic. Use the tools and search methods provided to research the user's input topic. For context,\n",
    "<Task>\n",
    "Your job is to use tools and search methods to find information that can answer the question that a user asks.\n",
    "You can use any of the tools provided to you to find resources that can help answer the research question. You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.\n",
    "</Task>\n",
    "\n",
    "<Tool Calling Guidelines>\n",
    "- Make sure you review all of the tools you have available to you, match the tools to the user's request, and select the tool that is most likely to be the best fit.\n",
    "- In each iteration, select the BEST tool for the job, this may or may not be general websearch.\n",
    "- When selecting the next tool to call, make sure that you are calling tools with arguments that you have not already tried.\n",
    "- Tool calling is costly, so be sure to be very intentional about what you look up. Some of the tools may have implicit limitations. As you call tools, feel out what these limitations are, and adjust your tool calls accordingly.\n",
    "- This could mean that you need to call a different tool, or that you should call \"ResearchComplete\", e.g. it's okay to recognize that a tool has limitations and cannot do what you need it to.\n",
    "- Don't mention any tool limitations in your output, but adjust your tool calls accordingly.\n",
    "\n",
    "<Tool Calling Guidelines>\n",
    "\n",
    "<Criteria for Finishing Research>\n",
    "- In addition to tools for research, you will also be given a special \"ResearchComplete\" tool. This tool is used to indicate that you are done with your research.\n",
    "- The user will give you a sense of how much effort you should put into the research. This does not translate ~directly~ to the number of tool calls you should make, but it does give you a sense of the depth of the research you should conduct.\n",
    "- DO NOT call \"ResearchComplete\" unless you are satisfied with your research.\n",
    "- One case where it's recommended to call this tool is if you see that your previous tool calls have stopped yielding useful information.\n",
    "</Criteria for Finishing Research>\n",
    "\n",
    "<Helpful Tips>\n",
    "1. If you haven't conducted any searches yet, start with broad searches to get necessary context and background information. Once you have some background, you can start to narrow down your searches to get more specific information.\n",
    "2. Different topics require different levels of research depth. If the question is broad, your research can be more shallow, and you may not need to iterate and call tools as many times.\n",
    "3. If the question is detailed, you may need to be more stingy about the depth of your findings, and you may need to iterate and call tools more times to get a fully detailed answer.\n",
    "</Helpful Tips>\n",
    "\n",
    "<Critical Reminders>\n",
    "- You MUST conduct research using web search or a different tool before you are allowed to call \"ResearchComplete\"! You cannot call \"ResearchComplete\" without conducting research first!\n",
    "- Do not repeat or summarize your research findings unless the user explicitly asks you to do so. Your main job is to call tools. You should call tools until you are satisfied with the research findings, and then call \"ResearchComplete\".\n",
    "</Critical Reminders>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def supervisor(state: SupervisorState, config: RunnableConfig):\n",
    "    Command[Literal[\"supervisor_tool\"]]\n",
    "    config = Configuration.from_runnable_config(config)\n",
    "    research_model_config = {\n",
    "        \"model\": config.research_model,\n",
    "        \"max_tokens\": config.research_model_max_tokens,\n",
    "        # \"api_key\": config.research_model_api_key,\n",
    "        \"tags\": [\"langsmith:nostream\"],\n",
    "    }\n",
    "\n",
    "    lead_research_tool = [ConductResearch, ResearchComplete]\n",
    "    research_model = (\n",
    "        configurable_model.bind_tools(lead_research_tool)\n",
    "        .with_retry(stop_after_attempt=config.max_structured_output_retries)\n",
    "        .with_config(\n",
    "            research_model_config,\n",
    "        )\n",
    "    )\n",
    "    supervisor_message = state.get(StatesKeys.SUPERVISOR_MSGS.value, [])\n",
    "    response = await research_model.ainvoke(supervisor_message)\n",
    "    return Command(\n",
    "        goto=\"supervisor_tool\",\n",
    "        update={\n",
    "            StatesKeys.SUPERVISOR_MSGS.value: [response],\n",
    "            \"research_iterations\": state.get(\"research_iterations\", 0) + 1,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "async def supervisor_tool(state: SupervisorState, config: RunnableConfig) -> Command[Literal[\"supervisor\", \"__end__\"]]:\n",
    "    \"\"\"Supervisor tool.\"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    supervisor_messages = state.get(StatesKeys.SUPERVISOR_MSGS.value, [])\n",
    "    research_iterations = state.get(\"research_iterations\", 0)\n",
    "    most_recent_message = supervisor_messages[-1]\n",
    "\n",
    "    # Exit Criteria\n",
    "    # 1. we have exceeded our max guardrail research  iteration\n",
    "    # 2. No tool call were made by supervisor\n",
    "    # 3. The most recent message contain a ResearchComplete tool call and there is only one tool call in the message\n",
    "    exceeded_allowed_iterations = research_iterations >= configurable.max_research_iterations\n",
    "    no_tool_calls = not most_recent_message.tool_calls\n",
    "    research_complete_tool_call = any(\n",
    "        tool_call[\"name\"] == \"ResearchComplete\" for tool_call in most_recent_message.tool_calls\n",
    "    )\n",
    "    if exceeded_allowed_iterations or no_tool_calls or research_complete_tool_call:\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\n",
    "                \"notes\": get_notes_from_tool_calls(supervisor_messages),\n",
    "                \"research_brief\": state.get(\"research_brief\", \"\"),\n",
    "            },\n",
    "        )\n",
    "    # otherwise, continue with research\n",
    "    try:\n",
    "        all_conduct_research = [\n",
    "            tool_call for tool_call in most_recent_message.tool_calls if tool_call[\"name\"] == \"ConductResearch\"\n",
    "        ]\n",
    "        # Limit total concurrent research units/calls\n",
    "        conduct_research_calls = all_conduct_research[: configurable.max_concurrent_research_units]\n",
    "        overflow_conduct_research_calls = all_conduct_research[configurable.max_concurrent_research_units :]\n",
    "\n",
    "        coros = [\n",
    "            researcher_subgraph.ainvoke(\n",
    "                {\n",
    "                    \"research_messages\": [\n",
    "                        SystemMessage(content=RESEARCH_SYSTEM_PROMPT),\n",
    "                        HumanMessage(content=tool_call[\"args\"][\"research_topic\"]),\n",
    "                    ],\n",
    "                    \"research_topic\": tool_call[\"args\"][\"research_topic\"],\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "            for tool_call in conduct_research_calls\n",
    "        ]\n",
    "        tool_results = await asyncio.gather(*coros)\n",
    "        tool_messages = [\n",
    "            ToolMessage(\n",
    "                content=observation.get(\n",
    "                    \"compressed_research\",\n",
    "                    \"Error synthesizing research report: Maximum retries exceeded\",\n",
    "                ),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "            for observation, tool_call in zip(tool_results, conduct_research_calls, strict=False)\n",
    "        ]\n",
    "        # Handle any tool calls made > max_concurrent_research_units\n",
    "        for overflow_conduct_research_call in overflow_conduct_research_calls:\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=f\"Error: Did not run this research as you have already exceeded the maximum number of concurrent research units. Please try again with {configurable.max_concurrent_research_units} or fewer research units.\",\n",
    "                    name=\"ConductResearch\",\n",
    "                    tool_call_id=overflow_conduct_research_call[\"id\"],\n",
    "                ),\n",
    "            )\n",
    "        raw_notes_concat = \"\\n\".join([\"\\n\".join(observation.get(\"raw_notes\", [])) for observation in tool_results])\n",
    "        return Command(\n",
    "            goto=\"supervisor\",\n",
    "            update={StatesKeys.SUPERVISOR_MSGS.value: tool_messages, \"raw_notes\": [raw_notes_concat]},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # import traceback\n",
    "\n",
    "        # print(traceback.format_exc())\n",
    "        if is_token_limit_exceeded(e, configurable.research_model):\n",
    "            print(f\"Token limit exceeded while reflecting: {e}\")\n",
    "        else:\n",
    "            print(f\"Other error in reflection phase: {e}\")\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\n",
    "                \"notes\": get_notes_from_tool_calls(supervisor_messages),\n",
    "                \"research_brief\": state.get(\"research_brief\", \"\"),\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "supervisor_builder = StateGraph(SupervisorState, config_schema=Configuration)\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)\n",
    "supervisor_builder.add_node(\"supervisor_tool\", supervisor_tool)\n",
    "supervisor_builder.add_edge(START, \"supervisor\")\n",
    "supervisor_builder.add_edge(\"supervisor_tool\", END)\n",
    "supervisor_graph = supervisor_builder.compile(name=\"Supervisor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571d79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    #  HumanMessage(content=PROJECT_IDEA),\n",
    "    AIMessage(\"\"\" 1. **Target Audience and Purpose:**\n",
    "   - You mentioned this is a personal project for fun and learning, as well as to showcase your skills to potential interviewers. Is there a specific aspect of the project you want to highlight to interviewers (e.g., AI capabilities, UI design, integration skills)?\n",
    "2. **Image Processing:**\n",
    "   - For the feature that takes pictures of handwritten notes, do you have any specific requirements or preferences for the image capture process (e.g., camera integration, file upload)?\n",
    "   - Are there any particular challenges you anticipate with processing equations and block diagrams, and do you have any existing tools or libraries in mind to handle these?\n",
    "3. **Integration with Notion:**\n",
    "   - Could you elaborate on how you envision the integration with Notion? For example, do you have a specific structure or organization in mind for the notes within Notion?\n",
    "   - Are there any specific Notion API functionalities you plan to use or any constraints you are aware of?\n",
    "4. **LangGraph and UI:**\n",
    "   - You mentioned using LangGraph's prebuilt UI. Could you provide more details on how you plan to utilize it? Are there any customizations or additional features you want to add to the UI?\n",
    "5. **Multi-Agent System:**\n",
    "   - You are considering a multi-agent system. Could you describe how you envision the interaction between the agents, and what specific roles each agent will play?\n",
    "6. **MVP and Timeline:**\n",
    "   - For the MVP, you aim to convert images to text and format them properly within two weeks. Are there any specific milestones or deliverables you have in mind for this initial phase?\n",
    "7. **Future Development:**\n",
    "   - You mentioned the possibility of developing a simple Android app using Flutter later on. Do you have any specific features or functionalities in mind for the mobile version?\n",
    " \"\"\"),\n",
    "    HumanMessage(\n",
    "        content=\"\"\"\n",
    "1. nothing specific, but good working app, because i am going to use it regularly\n",
    "2. for the time being we can go with file upload and when i decide to make android app using flutter then we can integrate images directly from the camera\n",
    "3. I am thinking to fetch all the pages and sub pages and find pages with name and create a sub page and add notes over there. for example if topic is about optimizer then i will find page with deep learning and create a new sub page and add notes over there.\n",
    "4. No, first i want to use only langgraph UI, it is really nice and easy to use\n",
    "5. I am also thinking to use multi agent system one agent for image to text conversion and 2nd agent for text to notion or formatting (markdown)\n",
    "6. no nothing specific milestone or anything\n",
    "7. no just simple camera app with little instruction/chat box for Agent if needed\n",
    "\"\"\",\n",
    "    ),\n",
    "    AIMessage(\"\"\"1. **App Usage and Quality:**\n",
    "   - You want a reliable and well-functioning app since you'll be using it regularly. Are there any specific performance or usability criteria you have in mind to ensure it meets your needs?\n",
    "2. **Image Capture and Processing:**\n",
    "   - For now, you prefer to start with a file upload feature for images. When you transition to an Android app using Flutter, do you have any specific requirements for how the camera integration should work?\n",
    "3. **Notion Integration:**\n",
    "   - You plan to fetch pages and sub-pages in Notion, find specific pages by name, and create sub-pages to add notes. Could you clarify if you have a specific naming convention or criteria for identifying these pages?\n",
    "   - How do you envision the notes being structured or formatted once they are added to Notion?\n",
    "4. **LangGraph UI:**\n",
    "   - You are satisfied with using LangGraph's UI as it is. Are there any specific features or interactions within the UI that are crucial for your workflow?\n",
    "5. **Multi-Agent System:**\n",
    "   - You plan to use one agent for image-to-text conversion and another for text formatting and integration with Notion. Could you describe any specific interactions or data flow between these agents?\n",
    "6. **Milestones and Timeline:**\n",
    "   - While you don't have specific milestones, do you have a rough timeline or order of priorities for implementing different features?\n",
    "7. **Future Android App:**\n",
    "   - For the future Android app, you mentioned a simple camera app with a chat box for agent instructions. Are there any additional features or user interactions you envision for this app?\n",
    "Please let me know if there are any other aspects of the project you'd like to discuss or clarify further.\n",
    " \"\"\"),\n",
    "    HumanMessage(\n",
    "        content=\"1. no, i have not thought about it yet, we can check after first MVP is done\"\n",
    "        \"2.i will use langgraph UI to upload the images for the time being\"\n",
    "        \"3. simple chat ui with upload image button and text input box\"\n",
    "        \"4. No not yet, because i have not use it. i will see once we reach to that point\"\n",
    "        \"5. first agent will convert image to text and send this text information to 2nd agent and 2nd agent will format it and send it to notion\"\n",
    "        \"6. no milestone or anything\"\n",
    "        \"7.no nothing\",\n",
    "    ),\n",
    "]\n",
    "PROJECT_IDEA = (\n",
    "    \"create plan to develop Agentic AI note taking app using langgraph for my personal use (personal project for fun and learning) and i also want to show off my skills to my potential interviewer to get hired. it should do following \"\n",
    "    \"1. take pictures of hand-written notes \"\n",
    "    \"2. it will automatically format the hand-written notes (it might contains equations and block diagrams) \"\n",
    "    \"3. find proper section (if section found then create sub page or create a new page) in my notion \"\n",
    "    \"4. add this notes with proper format\"\n",
    "    \"5. i want to use LangGraph's pre-build UI for interaction from PC\"\n",
    "    \"6. for MVP (which can convert image to text and format it properly) in 2 weeks\"\n",
    "    \"7. I am also thinking to use multi agent system one agent for image to text conversion and 2nd agent for text to notion or formatting (markdown)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent.utils import get_today_str\n",
    "\n",
    "research_brief = \"\"\"\n",
    "How can I develop an agentic AI-powered note-taking application leveraging LangGraph for my personal use, with features including image-to-text conversion, formatting, and integration with Notion, while showcasing my skills? I am seeking to:\n",
    "1. Employ a robust Optical Character Recognition (OCR) solution for converting handwritten notes, potentially containing equations and block diagrams, into digital text\n",
    "   - Specific OCR tool preferences are currently open-ended; the research should explore both existing solutions and state-of-the-art alternatives.\n",
    "2. Customize LangGraph's prebuilt UI for seamless interaction from a PC without any specific customization specifications, leaving room for creative design choices.\n",
    "3. Develop a seamless interface with Notion, utilizing either existing APIs or custom integration methods.\n",
    "   - The research should identify the best method for ensuring efficient data management and retrieval in Notion.\n",
    "4. Implement a multi-agent system where one agent focuses on image-to-text conversion and another takes charge of text integration and formatting using markdown.\n",
    "   - The interaction dynamics between agents require exploration to ensure efficiency and smooth operation.\n",
    "5. Deliver an initial MVP capable of converting images to well-formatted text within 2 weeks.\n",
    "6. Maintain an open architectural design, allowing flexibility in programming languages and frameworks, as no constraints have been specified.\n",
    "7. Enhance the application to demonstrate my capabilities to potential employers, focusing on cutting-edge approach and effective solutions.\n",
    "\"\"\"\n",
    "LEAD_RESEARCHER_PROMPT = \"\"\"You are a research supervisor. Your job is to conduct research by calling the \"ConductResearch\" tool. For context, today's date is {date}.\n",
    "\n",
    "<Task>\n",
    "Your focus is to call the \"ConductResearch\" tool to conduct research against the overall research question passed in by the user.\n",
    "When you are completely satisfied with the research findings returned from the tool calls, then you should call the \"ResearchComplete\" tool to indicate that you are done with your research.\n",
    "</Task>\n",
    "\n",
    "<Instructions>\n",
    "1. When you start, you will be provided a research question from a user.\n",
    "2. You should immediately call the \"ConductResearch\" tool to conduct research for the research question. You can call the tool up to {max_concurrent_research_units} times in a single iteration.\n",
    "3. Each ConductResearch tool call will spawn a research agent dedicated to the specific topic that you pass in. You will get back a comprehensive report of research findings on that topic.\n",
    "4. Reason carefully about whether all of the returned research findings together are comprehensive enough for a detailed report to answer the overall research question.\n",
    "5. If there are important and specific gaps in the research findings, you can then call the \"ConductResearch\" tool again to conduct research on the specific gap.\n",
    "6. Iteratively call the \"ConductResearch\" tool until you are satisfied with the research findings, then call the \"ResearchComplete\" tool to indicate that you are done with your research.\n",
    "7. Don't call \"ConductResearch\" to synthesize any information you've gathered. Another agent will do that after you call \"ResearchComplete\". You should only call \"ConductResearch\" to research net new topics and get net new information.\n",
    "</Instructions>\n",
    "\n",
    "\n",
    "<Important Guidelines>\n",
    "**The goal of conducting research is to get information, not to write the final report. Don't worry about formatting!**\n",
    "- A separate agent will be used to write the final report.\n",
    "- Do not grade or worry about the format of the information that comes back from the \"ConductResearch\" tool. It's expected to be raw and messy. A separate agent will be used to synthesize the information once you have completed your research.\n",
    "- Only worry about if you have enough information, not about the format of the information that comes back from the \"ConductResearch\" tool.\n",
    "- Do not call the \"ConductResearch\" tool to synthesize information you have already gathered.\n",
    "\n",
    "**Parallel research saves the user time, but reason carefully about when you should use it**\n",
    "- Calling the \"ConductResearch\" tool multiple times in parallel can save the user time.\n",
    "- You should only call the \"ConductResearch\" tool multiple times in parallel if the different topics that you are researching can be researched independently in parallel with respect to the user's overall question.\n",
    "- This can be particularly helpful if the user is asking for a comparison of X and Y, if the user is asking for a list of entities that each can be researched independently, or if the user is asking for multiple perspectives on a topic.\n",
    "- Each research agent needs to be provided all of the context that is necessary to focus on a sub-topic.\n",
    "- Do not call the \"ConductResearch\" tool more than {max_concurrent_research_units} times at once. This limit is enforced by the user. It is perfectly fine, and expected, that you return less than this number of tool calls.\n",
    "- If you are not confident in how you can parallelize research, you can call the \"ConductResearch\" tool a single time on a more general topic in order to gather more background information, so you have more context later to reason about if it's necessary to parallelize research.\n",
    "- Each parallel \"ConductResearch\" linearly scales cost. The benefit of parallel research is that it can save the user time, but carefully think about whether the additional cost is worth the benefit.\n",
    "- For example, if you could search three clear topics in parallel, or break them each into two more subtopics to do six total in parallel, you should think about whether splitting into smaller subtopics is worth the cost. The researchers are quite comprehensive, so it's possible that you could get the same information with less cost by only calling the \"ConductResearch\" tool three times in this case.\n",
    "- Also consider where there might be dependencies that cannot be parallelized. For example, if asked for details about some entities, you first need to find the entities before you can research them in detail in parallel.\n",
    "\n",
    "**Different questions require different levels of research depth**\n",
    "- If a user is asking a broader question, your research can be more shallow, and you may not need to iterate and call the \"ConductResearch\" tool as many times.\n",
    "- If a user uses terms like \"detailed\" or \"comprehensive\" in their question, you may need to be more stingy about the depth of your findings, and you may need to iterate and call the \"ConductResearch\" tool more times to get a fully detailed answer.\n",
    "\n",
    "**Research is expensive**\n",
    "- Research is expensive, both from a monetary and time perspective.\n",
    "- As you look at your history of tool calls, as you have conducted more and more research, the theoretical \"threshold\" for additional research should be higher.\n",
    "- In other words, as the amount of research conducted grows, be more stingy about making even more follow-up \"ConductResearch\" tool calls, and more willing to call \"ResearchComplete\" if you are satisfied with the research findings.\n",
    "- You should only ask for topics that are ABSOLUTELY necessary to research for a comprehensive answer.\n",
    "- Before you ask about a topic, be sure that it is substantially different from any topics that you have already researched. It needs to be substantially different, not just rephrased or slightly different. The researchers are quite comprehensive, so they will not miss anything.\n",
    "- When you call the \"ConductResearch\" tool, make sure to explicitly state how much effort you want the sub-agent to put into the research. For background research, you may want it to be a shallow or small effort. For critical topics, you may want it to be a deep or large effort. Make the effort level explicit to the researcher.\n",
    "</Important Guidelines>\n",
    "\n",
    "\n",
    "<Crucial Reminders>\n",
    "- If you are satisfied with the current state of research, call the \"ResearchComplete\" tool to indicate that you are done with your research.\n",
    "- Calling ConductResearch in parallel will save the user time, but you should only do this if you are confident that the different topics that you are researching are independent and can be researched in parallel with respect to the user's overall question.\n",
    "- You should ONLY ask for topics that you need to help you answer the overall research question. Reason about this carefully.\n",
    "- When calling the \"ConductResearch\" tool, provide all context that is necessary for the researcher to understand what you want them to research. The independent researchers will not get any context besides what you write to the tool each time, so make sure to provide all context to it.\n",
    "- This means that you should NOT reference prior tool call results or the research brief when calling the \"ConductResearch\" tool. Each input to the \"ConductResearch\" tool should be a standalone, fully explained topic.\n",
    "- Do NOT use acronyms or abbreviations in your research questions, be very clear and specific.\n",
    "</Crucial Reminders>\n",
    "\n",
    "With all of the above in mind, call the ConductResearch tool to conduct research on specific topics, OR call the \"ResearchComplete\" tool to indicate that you are done with your research.\n",
    "\"\"\"\n",
    "\n",
    "sys_msg = SystemMessage(content=LEAD_RESEARCHER_PROMPT.format(date=get_today_str(), max_concurrent_research_units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c8a184",
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m supervisor_graph.astream(\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         StatesKeys.SUPERVISOR_MSGS.value: [sys_msg, HumanMessage(content=research_brief)],\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresearch_brief\u001b[39m\u001b[33m\"\u001b[39m: research_brief,\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# \"research_topic\": PROJECT_IDEA,\u001b[39;00m\n\u001b[32m      6\u001b[39m     },\n\u001b[32m      7\u001b[39m     stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m ):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2768\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2766\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2767\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2768\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2769\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2770\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2771\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2772\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2773\u001b[39m ):\n\u001b[32m   2774\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2775\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2776\u001b[39m         stream_mode,\n\u001b[32m   2777\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2780\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2781\u001b[39m     ):\n\u001b[32m   2782\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36msupervisor_tool\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    101\u001b[39m overflow_conduct_research_calls = all_conduct_research[configurable.max_concurrent_research_units :]\n\u001b[32m    103\u001b[39m coros = [\n\u001b[32m    104\u001b[39m     researcher_subgraph.ainvoke(\n\u001b[32m    105\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m conduct_research_calls\n\u001b[32m    115\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m tool_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*coros)\n\u001b[32m    117\u001b[39m tool_messages = [\n\u001b[32m    118\u001b[39m     ToolMessage(\n\u001b[32m    119\u001b[39m         content=observation.get(\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m observation, tool_call \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tool_results, conduct_research_calls, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    127\u001b[39m ]\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Handle any tool calls made > max_concurrent_research_units\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2920\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2917\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2918\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2920\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2921\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2922\u001b[39m     config,\n\u001b[32m   2923\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   2926\u001b[39m     print_mode=print_mode,\n\u001b[32m   2927\u001b[39m     output_keys=output_keys,\n\u001b[32m   2928\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2929\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2930\u001b[39m     **kwargs,\n\u001b[32m   2931\u001b[39m ):\n\u001b[32m   2932\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2933\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2768\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2766\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2767\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2768\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2769\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2770\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2771\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2772\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2773\u001b[39m ):\n\u001b[32m   2774\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2775\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2776\u001b[39m         stream_mode,\n\u001b[32m   2777\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2780\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2781\u001b[39m     ):\n\u001b[32m   2782\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/src/agent/researcher_agent.py:55\u001b[39m, in \u001b[36mresearch_agent\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m     40\u001b[39m research_model_config = {\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: config.research_model,\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: config.research_model_max_tokens,\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# \"api_key\": config.research_model_api_key,\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mlangsmith:nostream\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     45\u001b[39m }\n\u001b[32m     47\u001b[39m research_model = (\n\u001b[32m     48\u001b[39m     configurable_model.bind_tools(\n\u001b[32m     49\u001b[39m         tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     .with_config(research_model_config)\n\u001b[32m     53\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m research_model.ainvoke(\n\u001b[32m     56\u001b[39m     research_msgs,\n\u001b[32m     57\u001b[39m )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Command(\n\u001b[32m     60\u001b[39m     goto=\u001b[33m\"\u001b[39m\u001b[33mresearch_tools\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m     update={\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     },\n\u001b[32m     65\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5447\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5442\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5445\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5446\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5448\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5449\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5450\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5451\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/runnables/retry.py:225\u001b[39m, in \u001b[36mRunnableRetry.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Input, config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any\n\u001b[32m    224\u001b[39m ) -> Output:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acall_with_config(\u001b[38;5;28mself\u001b[39m._ainvoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:1990\u001b[39m, in \u001b[36mRunnable._acall_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1986\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1987\u001b[39m         coro = acall_func_with_variable_args(\n\u001b[32m   1988\u001b[39m             func, input_, config, run_manager, **kwargs\n\u001b[32m   1989\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1990\u001b[39m         output: Output = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(coro, context)\n\u001b[32m   1991\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1992\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/runnables/retry.py:210\u001b[39m, in \u001b[36mRunnableRetry._ainvoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ainvoke\u001b[39m(\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    205\u001b[39m     input_: Input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     **kwargs: Any,\n\u001b[32m    209\u001b[39m ) -> Output:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_retrying(reraise=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    211\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[32m    212\u001b[39m             result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().ainvoke(\n\u001b[32m    213\u001b[39m                 input_,\n\u001b[32m    214\u001b[39m                 \u001b[38;5;28mself\u001b[39m._patch_config(config, run_manager, attempt.retry_state),\n\u001b[32m    215\u001b[39m                 **kwargs,\n\u001b[32m    216\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:166\u001b[39m, in \u001b[36mAsyncRetrying.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AttemptManager:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=\u001b[38;5;28mself\u001b[39m._retry_state)\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m do \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/runnables/retry.py:212\u001b[39m, in \u001b[36mRunnableRetry._ainvoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_retrying(reraise=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().ainvoke(\n\u001b[32m    213\u001b[39m             input_,\n\u001b[32m    214\u001b[39m             \u001b[38;5;28mself\u001b[39m._patch_config(config, run_manager, attempt.retry_state),\n\u001b[32m    215\u001b[39m             **kwargs,\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt.retry_state.outcome \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attempt.retry_state.outcome.failed:\n\u001b[32m    218\u001b[39m         attempt.retry_state.set_result(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5447\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5442\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5445\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5446\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5448\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5449\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5450\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5451\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:687\u001b[39m, in \u001b[36m_ConfigurableModel.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    682\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    683\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m    684\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    685\u001b[39m     **kwargs: Any,\n\u001b[32m    686\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model(config).ainvoke(\u001b[38;5;28minput\u001b[39m, config=config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5447\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5442\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5445\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5446\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5448\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5449\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5450\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5451\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:400\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    392\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     **kwargs: Any,\n\u001b[32m    398\u001b[39m ) -> BaseMessage:\n\u001b[32m    399\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    401\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    402\u001b[39m         stop=stop,\n\u001b[32m    403\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    404\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    405\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    407\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    408\u001b[39m         **kwargs,\n\u001b[32m    409\u001b[39m     )\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:974\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    967\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    971\u001b[39m     **kwargs: Any,\n\u001b[32m    972\u001b[39m ) -> LLMResult:\n\u001b[32m    973\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    975\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    976\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:894\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    881\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    882\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    883\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    888\u001b[39m     run_id=run_id,\n\u001b[32m    889\u001b[39m )\n\u001b[32m    891\u001b[39m input_messages = [\n\u001b[32m    892\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    893\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    895\u001b[39m     *[\n\u001b[32m    896\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    897\u001b[39m             m,\n\u001b[32m    898\u001b[39m             stop=stop,\n\u001b[32m    899\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    900\u001b[39m             **kwargs,\n\u001b[32m    901\u001b[39m         )\n\u001b[32m    902\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    903\u001b[39m     ],\n\u001b[32m    904\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    905\u001b[39m )\n\u001b[32m    906\u001b[39m exceptions = []\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1100\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1098\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1101\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1104\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1367\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1365\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(**payload)\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[32m   1369\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._create_chat_result, response, generation_info\n\u001b[32m   1370\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/openai/_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/openai/_base_client.py:1526\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1524\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1526\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1527\u001b[39m         request,\n\u001b[32m   1528\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1529\u001b[39m         **kwargs,\n\u001b[32m   1530\u001b[39m     )\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1532\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/anyio/streams/tls.py:219\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/anyio/streams/tls.py:162\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    160\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/project-planning-genie/.venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/asyncio/locks.py:213\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "async for event in supervisor_graph.astream(\n",
    "    {\n",
    "        StatesKeys.SUPERVISOR_MSGS.value: [sys_msg, HumanMessage(content=research_brief)],\n",
    "        \"research_brief\": research_brief,\n",
    "        # \"research_topic\": PROJECT_IDEA,\n",
    "    },\n",
    "    stream_mode=\"update\",\n",
    "):\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768de548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42324d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-planning-genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
