{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable reloading\n",
    "%load_ext autoreload\n",
    "# all the modules should be reloaded before executing the code\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b835e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "import rootutils\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Command, interrupt\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel\n",
    "\n",
    "rootutils.setup_root(search_from=str(Path.cwd().parent), indicator=[\".git\", \"pyproject.toml\"], pythonpath=True)\n",
    "from src.agent.my_mcps import mcp_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427957a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for context engineering notebooks.\n",
    "\"\"\"\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "import json\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def format_message_content(message):\n",
    "    \"\"\"Convert message content to displayable string\"\"\"\n",
    "    if isinstance(message.content, str):\n",
    "        return message.content\n",
    "    elif isinstance(message.content, list):\n",
    "        # Handle complex content like tool calls\n",
    "        parts = []\n",
    "        for item in message.content:\n",
    "            if item.get(\"type\") == \"text\":\n",
    "                parts.append(item[\"text\"])\n",
    "            elif item.get(\"type\") == \"tool_use\":\n",
    "                parts.append(f\"\\nðŸ”§ Tool Call: {item['name']}\")\n",
    "                parts.append(f\"   Args: {json.dumps(item['input'], indent=2)}\")\n",
    "        return \"\\n\".join(parts)\n",
    "    else:\n",
    "        return str(message.content)\n",
    "\n",
    "\n",
    "def format_messages(messages):\n",
    "    \"\"\"Format and display a list of messages with Rich formatting\"\"\"\n",
    "    for m in messages:\n",
    "        msg_type = m.__class__.__name__.replace(\"Message\", \"\")\n",
    "        content = format_message_content(m)\n",
    "\n",
    "        if msg_type == \"Human\":\n",
    "            console.print(Panel(content, title=\"ðŸ§‘ Human\", border_style=\"blue\"))\n",
    "        elif msg_type == \"Ai\":\n",
    "            console.print(Panel(content, title=\"ðŸ¤– Assistant\", border_style=\"green\"))\n",
    "        elif msg_type == \"Tool\":\n",
    "            console.print(Panel(content, title=\"ðŸ”§ Tool Output\", border_style=\"yellow\"))\n",
    "        else:\n",
    "            console.print(Panel(content, title=f\"ðŸ“ {msg_type}\", border_style=\"white\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ef5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class States(MessagesState):\n",
    "    \"\"\"State of conversation between Agent and User.\"\"\"\n",
    "\n",
    "    # messages: Annotated[list[BaseMessage], add_messages] = []\n",
    "\n",
    "\n",
    "protected_tools: list[str] = [\"create_directory\", \"edit_file\", \"write_file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef044dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(connections=mcp_config[\"mcpServers\"])\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df15e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\").bind_tools(tools)\n",
    "\n",
    "# llm = ChatPerplexity(model=\"sonar-pro\", temperature=0)\n",
    "llm = ChatOllama(model=\"qwen3:8b\", temperature=0).bind_tools(tools)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4.1-mini-2025-04-14\",\n",
    "#     temperature=0.1,\n",
    "# ).bind_tools(tools)\n",
    "\n",
    "llm.invoke(\"hii how are you ? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "def human_tool_review_node(\n",
    "    state: States,\n",
    ") -> Command[Literal[\"tools\", \"assistant_node\"]]:\n",
    "    \"\"\"Node is a placeholder for the human to review the final report generation process to verify proper tool call checks before tools are called by the agent.\"\"\"\n",
    "    print(\"[INFO] human_tool_review_node called\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Ensure we have a valid AI message with tool calls\n",
    "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
    "        msg = \"human_tool_review_node called without valid tool calls\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    tool_call = last_message.tool_calls[-1]\n",
    "\n",
    "    # Stop graph execution and wait for human input\n",
    "    human_review: dict = interrupt(\n",
    "        {\"message\": \"Your input is required for the following tool:\", \"tool_call\": tool_call},\n",
    "    )\n",
    "    review_action = human_review.get(\"action\")\n",
    "    review_data = human_review.get(\"data\")\n",
    "\n",
    "    if review_action == \"accept\":\n",
    "        return Command(\n",
    "            goto=\"tools\",\n",
    "        )\n",
    "    return Command(\n",
    "        goto=\"assistant_node\",\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=review_data),\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def assistant_node(state: States) -> States:\n",
    "    print(\"[INFO] assistant_node called\")\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant. You have access to the local filesystem but only within an approved directory. The approved directory is /projects/workspace and all paths must begin with /projects/workspace/. You must use /project/workspace/generated_example directory. if directory does not exists then create it and then give a good name of the <file_name>.md file (for example sw_design.md) and save the generated report in /project/workspace/generated_example directory.\",\n",
    "            ),\n",
    "            *state[\"messages\"],\n",
    "        ],\n",
    "    )\n",
    "    state[\"messages\"] = [*state[\"messages\"], response]\n",
    "    return state\n",
    "\n",
    "\n",
    "def router(state: States) -> str:\n",
    "    print(\"[INFO] router called\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        if any(tool_call[\"name\"] in protected_tools for tool_call in last_message.tool_calls):\n",
    "            return \"human_tool_review_node\"\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(States)\n",
    "\n",
    "builder.add_node(\"assistant_node\", assistant_node)\n",
    "builder.add_node(\"human_tool_review_node\", human_tool_review_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"assistant_node\")\n",
    "builder.add_conditional_edges(\"assistant_node\", router, [\"tools\", \"human_tool_review_node\", END])\n",
    "builder.add_edge(\"tools\", \"assistant_node\")\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda61e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Generate a report on the project planning process. I don't know where to start, i want to create simple chatbot using langgraph. i am testing that you can use filesystem or not. simply generate a report without asking further question.\",\n",
    "        ),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaab80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "\n",
    "async def test2():\n",
    "    async for stream_mode, chunk in graph.astream(_input, thread, stream_mode=[\"updates\", \"messages\"]):\n",
    "        # format_messages(event[\"messages\"])  # event[\"messages\"][-1].pretty_print()\n",
    "        # if stream_mode == \"updates\":\n",
    "        #     graph_name = list(chunk.keys())[0]\n",
    "        #     print(graph_name)\n",
    "        #     message = chunk[graph_name][\"messages\"][-1]\n",
    "        #     yield message\n",
    "        if stream_mode == \"messages\":  # TODO: grab graph name here and use token streaming from messages\n",
    "            message, metadata = chunk\n",
    "            subgraph_name = metadata[\"langgraph_node\"]\n",
    "            if isinstance(message, AIMessageChunk):\n",
    "                if message.response_metadata:\n",
    "                    finish_reason = message.response_metadata.get(\"finish_reason\", \"\")\n",
    "                    if finish_reason == \"tool_calls\":\n",
    "                        yield \"\\n\\n\", subgraph_name\n",
    "\n",
    "                if message.tool_call_chunks:\n",
    "                    tool_chunk = message.tool_call_chunks[0]\n",
    "\n",
    "                    tool_name = tool_chunk.get(\"name\", \"\")\n",
    "                    args = tool_chunk.get(\"args\", \"\")\n",
    "\n",
    "                    if tool_name:\n",
    "                        tool_call_str = f\"\\n\\n< TOOL CALL: {tool_name} >\\n\\n\"\n",
    "                    if args:\n",
    "                        tool_call_str = args\n",
    "\n",
    "                    yield tool_call_str, subgraph_name\n",
    "                else:\n",
    "                    yield message.content, subgraph_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d971a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import node\n",
    "\n",
    "\n",
    "async def test2():\n",
    "    async for chunk in graph.astream(_input, thread, stream_mode=\"updates\"):\n",
    "        node_name = next(iter(chunk.keys()))\n",
    "        if node_name == \"assistant_node\":\n",
    "            msg = chunk[node_name]\n",
    "            msg = msg[\"messages\"][-1].content\n",
    "            yield f\"{node_name}: {msg}\"\n",
    "        elif node_name == \"__interrupt__\":\n",
    "            total_interrupts = []\n",
    "            for _interrupts in chunk[node_name]:\n",
    "                msg = _interrupts.value.get(\"message\")\n",
    "                tool_call = _interrupts.value.get(\"tool_call\")\n",
    "                tool_name = tool_call.get(\"name\")\n",
    "                tool_args = tool_call.get(\"args\")\n",
    "                formatted_string = \"\\n\".join(f\"{key.capitalize()}: {value}\" for key, value in tool_args.items())\n",
    "                tool_call_str = f\"\\n{msg}\\n\\n< TOOL CALL: tool_name: {tool_name} >\\ntool_arg: {formatted_string}\"\n",
    "                total_interrupts.append(tool_call_str)\n",
    "            yield \"\\n\\n\".join(total_interrupts)\n",
    "\n",
    "        # if \"messages\" in event:\n",
    "        #     latest_message = event[\"messages\"][-1]\n",
    "        #     if hasattr(latest_message, \"tool_calls\") and latest_message.tool_calls:\n",
    "        #         print(latest_message.tool_calls)\n",
    "        #         yield latest_message.tool_calls\n",
    "        #     yield latest_message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for response in test2():\n",
    "    # if isinstance(response, dict):\n",
    "    print(response)\n",
    "    # keys = list(response.keys())\n",
    "    # graph_name = keys[0]\n",
    "    # print(graph_name)\n",
    "    # message = response[graph_name][\"messages\"][-1]\n",
    "    # print(message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(config=thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_state = graph.get_state(thread).values\n",
    "# for m in new_state[\"messages\"]:\n",
    "#     m.pretty_print()\n",
    "# async for event in graph.astream(_input, thread, stream_mode=\"values\"):\n",
    "#     event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34790a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.update_state(\n",
    "#     thread,\n",
    "#     {\"messages\": [HumanMessage(content=\"accept\")]},\n",
    "# )\n",
    "# _input = {\"messages\": [HumanMessage(content=\"accept\")]}\n",
    "\n",
    "# new_state = graph.get_state(thread).values\n",
    "# for m in new_state[\"messages\"]:\n",
    "#     m.pretty_print()\n",
    "async for event in graph.astream(Command(resume={\"action\": \"accept\", \"data\": \"\"}), thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de45242",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in graph.astream(None, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5560693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model_shell = init_chat_model(\n",
    "#     configurable_fields=(\"model\", \"max_tokens\"),\n",
    "# )\n",
    "\n",
    "# report_generator_config = {\n",
    "#     \"model\": \"ollama:qwen3:8b\",\n",
    "# }\n",
    "# report_generator_model = model_shell.with_config(report_generator_config)\n",
    "# report_generator_model.invoke(\"hello world\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-planning-genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
