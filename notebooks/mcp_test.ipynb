{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e345ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable reloading\n",
    "%load_ext autoreload\n",
    "# all the modules should be reloaded before executing the code\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b835e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "import rootutils\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Command, interrupt\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel\n",
    "\n",
    "rootutils.setup_root(search_from=str(Path.cwd().parent), indicator=[\".git\", \"pyproject.toml\"], pythonpath=True)\n",
    "from src.agent.my_mcps import mcp_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427957a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for context engineering notebooks.\n",
    "\"\"\"\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "import json\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def format_message_content(message):\n",
    "    \"\"\"Convert message content to displayable string\"\"\"\n",
    "    if isinstance(message.content, str):\n",
    "        return message.content\n",
    "    elif isinstance(message.content, list):\n",
    "        # Handle complex content like tool calls\n",
    "        parts = []\n",
    "        for item in message.content:\n",
    "            if item.get(\"type\") == \"text\":\n",
    "                parts.append(item[\"text\"])\n",
    "            elif item.get(\"type\") == \"tool_use\":\n",
    "                parts.append(f\"\\n🔧 Tool Call: {item['name']}\")\n",
    "                parts.append(f\"   Args: {json.dumps(item['input'], indent=2)}\")\n",
    "        return \"\\n\".join(parts)\n",
    "    else:\n",
    "        return str(message.content)\n",
    "\n",
    "\n",
    "def format_messages(messages):\n",
    "    \"\"\"Format and display a list of messages with Rich formatting\"\"\"\n",
    "    for m in messages:\n",
    "        msg_type = m.__class__.__name__.replace(\"Message\", \"\")\n",
    "        content = format_message_content(m)\n",
    "\n",
    "        if msg_type == \"Human\":\n",
    "            console.print(Panel(content, title=\"🧑 Human\", border_style=\"blue\"))\n",
    "        elif msg_type == \"Ai\":\n",
    "            console.print(Panel(content, title=\"🤖 Assistant\", border_style=\"green\"))\n",
    "        elif msg_type == \"Tool\":\n",
    "            console.print(Panel(content, title=\"🔧 Tool Output\", border_style=\"yellow\"))\n",
    "        else:\n",
    "            console.print(Panel(content, title=f\"📝 {msg_type}\", border_style=\"white\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55ef5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class States(MessagesState):\n",
    "    \"\"\"State of conversation between Agent and User.\"\"\"\n",
    "\n",
    "    # messages: Annotated[list[BaseMessage], add_messages] = []\n",
    "\n",
    "\n",
    "protected_tools: list[str] = [\"create_directory\", \"edit_file\", \"write_file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef044dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(connections=mcp_config[\"mcpServers\"])\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df15e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! I am an AI assistant, so I don\\'t have feelings, but I\\'m here to help you with any questions or information you need.\\n\\nIf you meant your greeting in the context of the \"Hi, How Are You\" project or Daniel Johnston\\'s famous album, both are notable cultural references. The \"Hi, How Are You Project\" is a non-profit focused on mental health awareness, inspired by Daniel Johnston\\'s art and music[1][3]. If you have questions about mental health, Daniel Johnston, or anything else, feel free to ask!', additional_kwargs={'citations': ['https://www.hihowareyou.org', 'https://www.youtube.com/watch?v=AW2cmIIomac', 'https://en.wikipedia.org/wiki/Hi,_How_Are_You', 'https://open.spotify.com/album/2wZcpjsg8eNUVqY324mFu5', 'https://www.youtube.com/watch?v=j7asUTbtNWc'], 'search_results': [{'title': 'Hi, How Are You Project', 'url': 'https://www.hihowareyou.org', 'date': None, 'last_updated': '2025-08-03'}, {'title': 'Daniel Johnston - Hi, How Are You (Full Album, 1983)', 'url': 'https://www.youtube.com/watch?v=AW2cmIIomac', 'date': '2018-09-20', 'last_updated': '2025-06-29'}, {'title': 'Hi, How Are You', 'url': 'https://en.wikipedia.org/wiki/Hi,_How_Are_You', 'date': '2007-04-30', 'last_updated': '2025-06-22'}, {'title': 'Hi How Are You - Album by Daniel Johnston', 'url': 'https://open.spotify.com/album/2wZcpjsg8eNUVqY324mFu5', 'date': None, 'last_updated': '2024-01-26'}, {'title': 'Kenya Grace - Hey, Hi, How are you? (Official Lyric Video)', 'url': 'https://www.youtube.com/watch?v=j7asUTbtNWc', 'date': '2024-03-22', 'last_updated': '2025-07-06'}]}, response_metadata={'model_name': 'sonar-pro'}, id='run--ddc638b8-46ef-4797-b75e-b93601e03857-0', usage_metadata={'input_tokens': 6, 'output_tokens': 113, 'total_tokens': 119})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "\n",
    "# llm = ChatOllama(model=\"qwen3:14b\", temperature=0).bind_tools(tools)\n",
    "llm = ChatPerplexity(model=\"sonar-pro\", temperature=0)\n",
    "# llm = ChatOllama(model=\"qwen3:8b\", temperature=0).bind_tools(tools)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4.1-mini-2025-04-14\",\n",
    "#     temperature=0.1,\n",
    "# ).bind_tools(tools)\n",
    "\n",
    "llm.invoke(\"hii how are you ? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "def human_tool_review_node(\n",
    "    state: States,\n",
    ") -> Command[Literal[\"tools\", \"assistant_node\"]]:\n",
    "    \"\"\"Node is a placeholder for the human to review the final report generation process to verify proper tool call checks before tools are called by the agent.\"\"\"\n",
    "    print(\"[INFO] human_tool_review_node called\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Ensure we have a valid AI message with tool calls\n",
    "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
    "        msg = \"human_tool_review_node called without valid tool calls\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    tool_call = last_message.tool_calls[-1]\n",
    "\n",
    "    # Stop graph execution and wait for human input\n",
    "    human_review: dict = interrupt(\n",
    "        {\"message\": \"Your input is required for the following tool:\", \"tool_call\": tool_call},\n",
    "    )\n",
    "    review_action = human_review.get(\"action\")\n",
    "    review_data = human_review.get(\"data\")\n",
    "\n",
    "    if review_action == \"accept\":\n",
    "        return Command(\n",
    "            goto=\"tools\",\n",
    "        )\n",
    "    return Command(\n",
    "        goto=\"assistant_node\",\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=review_data),\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def assistant_node(state: States) -> States:\n",
    "    print(\"[INFO] assistant_node called\")\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant. You have access to the local filesystem but only within an approved directory. The approved directory is /projects/workspace and all paths must begin with /projects/workspace/. You must use /project/workspace/generated_example directory. if directory does not exists then create it and then give a good name of the <file_name>.md file (for example sw_design.md) and save the generated report in /project/workspace/generated_example directory.\",\n",
    "            ),\n",
    "            *state[\"messages\"],\n",
    "        ],\n",
    "    )\n",
    "    state[\"messages\"] = [*state[\"messages\"], response]\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def router(state: States) -> str:\n",
    "    print(\"[INFO] router called\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        if any(tool_call[\"name\"] in protected_tools for tool_call in last_message.tool_calls):\n",
    "            return \"human_tool_review_node\"\n",
    "        return \"tools\"\n",
    "\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(States)\n",
    "\n",
    "builder.add_node(\"assistant_node\", assistant_node)\n",
    "builder.add_node(\"human_tool_review_node\", human_tool_review_node)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"assistant_node\")\n",
    "builder.add_conditional_edges(\"assistant_node\", router, [\"tools\", \"human_tool_review_node\", END])\n",
    "builder.add_edge(\"tools\", \"assistant_node\")\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda61e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Generate a report on the project planning process. I don't know where to start, i want to create simple chatbot using langgraph. i am testing that you can use filesystem or not. simply generate a report without asking further question.\",\n",
    "        ),\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaab80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "async for event in graph.astream(_input, thread, stream_mode=\"update\"):\n",
    "    format_messages(event[\"messages\"])  # event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34790a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.update_state(\n",
    "#     thread,\n",
    "#     {\"messages\": [HumanMessage(content=\"accept\")]},\n",
    "# )\n",
    "_input = {\"messages\": [HumanMessage(content=\"accept\")]}\n",
    "\n",
    "# new_state = graph.get_state(thread).values\n",
    "# for m in new_state[\"messages\"]:\n",
    "#     m.pretty_print()\n",
    "async for event in graph.astream(Command(resume={\"action\": \"accept\", \"data\": \"\"}), thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de45242",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in graph.astream(None, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5560693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model_shell = init_chat_model(\n",
    "#     configurable_fields=(\"model\", \"max_tokens\"),\n",
    "# )\n",
    "\n",
    "# report_generator_config = {\n",
    "#     \"model\": \"ollama:qwen3:8b\",\n",
    "# }\n",
    "# report_generator_model = model_shell.with_config(report_generator_config)\n",
    "# report_generator_model.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62d1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-planning-genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
