{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14e2849",
   "metadata": {},
   "source": [
    "# Agent with Basic Functionality and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae026fe",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "https://www.youtube.com/watch?v=hvAPnpSfSGo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e32c5d",
   "metadata": {},
   "source": [
    "## States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4bd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ConductResearch(BaseModel):\n",
    "    \"\"\"Call this tool to conduct research on a specific topic.\"\"\"\n",
    "\n",
    "    research_topic: str = Field(\n",
    "        description=\"Project idea with detailed description (as much as information possible) by User\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ResearchComplete(BaseModel):\n",
    "    \"\"\"Call this tool to indicate that the research is complete.\"\"\"\n",
    "\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    Summary: str\n",
    "    key_excerpts: str\n",
    "\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    \"\"\"Call this tool to ask a clarification questions/infomartion to the user.\"\"\"\n",
    "\n",
    "    need_clarification: bool = Field(\n",
    "        description=\"Whether the user needs to be asked a clarification question.\",\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"The question to ask the user to clarify the report scope\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description=\"Verify message that we will start research after the user has provided the necessary information.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"Research questions to guide the research.\"\"\"\n",
    "\n",
    "    research_brief: str = Field(\n",
    "        description=\"A research question that will be used to guide the research.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53bb0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import MessageLikeRepresentation\n",
    "\n",
    "\n",
    "class AgentInputState(MessagesState):\n",
    "    \"\"\"Input state is only messages.\"\"\"\n",
    "\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"Agents States.\"\"\"\n",
    "\n",
    "    supervisor_message: Annotated[list[MessageLikeRepresentation], add_messages]\n",
    "    research_brief: str | None\n",
    "    raw_notes: Annotated[list[str] | None, add_messages] = None\n",
    "    notes: Annotated[list[str] | None, add_messages] = None\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937816c8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334be993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class SearchAPI(Enum):\n",
    "    \"\"\"Search APIs.\"\"\"\n",
    "\n",
    "    OPENAI = \"openai\"\n",
    "    TAVILY = \"tavily\"\n",
    "    DUCKDUCKGO = \"duckduckgo\"\n",
    "    GOOGLESEARCH = \"googlesearch\"\n",
    "    NONE = \"none\"\n",
    "\n",
    "\n",
    "class Defaults(Enum):\n",
    "    \"\"\"all Defaults settings.\"\"\"\n",
    "\n",
    "    RESEARCH_MODEL: str = \"ollama:qwen2.5:14b\"  # \"openai:gpt-4o\"\n",
    "    COMPRESSION_MODEL: str = \"ollama:qwen2.5:14b\"  # \"openai:gpt-4o-mini\"\n",
    "    SEARCH_API: SearchAPI = SearchAPI.TAVILY\n",
    "\n",
    "\n",
    "class Configuration(BaseModel):\n",
    "    \"\"\"Configuration for the Agent/App.\"\"\"\n",
    "\n",
    "    # --- Research Model ---\n",
    "    research_model: str = Field(\n",
    "        default=Defaults.RESEARCH_MODEL.value,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"text\",\n",
    "                \"default\": \"openai:gpt-4.1\",\n",
    "                \"description\": \"Model for conducting research. NOTE: Make sure your Researcher Model supports the selected search API.\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    research_model_max_tokens: int = Field(\n",
    "        default=10000,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\": 10000,\n",
    "                \"description\": \"Maximum output tokens for research model\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    # --- Compression Model ---\n",
    "    compression_model: str = Field(\n",
    "        default=Defaults.COMPRESSION_MODEL.value,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"text\",\n",
    "                \"default\": Defaults.COMPRESSION_MODEL.value,\n",
    "                \"description\": \"Model for compressing research findings from sub-agents. NOTE: Make sure your Compression Model supports the selected search API.\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    compression_model_max_tokens: int = Field(\n",
    "        default=8192,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\": 8192,\n",
    "                \"description\": \"Maximum output tokens for compression model\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    max_structured_output_retries: int = Field(\n",
    "        default=3,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"number\",\n",
    "                \"default\": 3,\n",
    "                \"min\": 1,\n",
    "                \"max\": 10,\n",
    "                \"description\": \"Maximum number of retries for structured output calls from models\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    allow_clarification: bool = Field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"default\": True,\n",
    "                \"description\": \"Whether to allow the researcher to ask the user clarifying questions before starting research\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    max_concurrent_research_unit: int = Field(\n",
    "        default=3,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"slider\",\n",
    "                \"default\": 3,\n",
    "                \"min\": 1,\n",
    "                \"max\": 20,\n",
    "                \"description\": \"Maximum number of research units to run concurrently. This will allow the researcher to use multiple sub-agents to conduct research. Note: with more concurrency, you may run into rate limits.\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    # Research Configuration\n",
    "    search_api: SearchAPI = Field(\n",
    "        default=Defaults.SEARCH_API.value,\n",
    "        metadata={\n",
    "            \"x_oap_ui_config\": {\n",
    "                \"type\": \"select\",\n",
    "                \"default\": \"tavily\",\n",
    "                \"options\": [\n",
    "                    {\"label\": \"Tavily\", \"value\": SearchAPI.TAVILY.value},\n",
    "                    {\n",
    "                        \"label\": \"OpenAI Native Web Search\",\n",
    "                        \"value\": SearchAPI.OPENAI.value,\n",
    "                    },\n",
    "                    {\"value\": \"none\", \"label\": \"None\"},\n",
    "                ],\n",
    "                \"description\": \"Search API to use for research. NOTE: Make sure your Researcher Model supports the selected search API.\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls,\n",
    "        config: RunnableConfig | None = None,\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = config.get(\"configurable\", {}) if config else {}\n",
    "        field_names = list(cls.model_fields.keys())\n",
    "        values: dict[str, Any] = {\n",
    "            field_name: os.environ.get(field_name.upper(), configurable.get(field_name)) for field_name in field_names\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v is not None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ced8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.chat_models.base._ConfigurableModel at 0x723edf8b8590>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Initialize a configurable model that we will use throughout the agent\n",
    "configurable_model = init_chat_model(\n",
    "    configurable_fields=(\"model\", \"max_tokens\", \"api_key\"),\n",
    ")\n",
    "configurable_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101bc4a",
   "metadata": {},
   "source": [
    "## Graph functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ed2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_with_user_instructions = \"\"\"\n",
    "You are an expert AI Software Architect and with more than 10 years of SW development and design experience. Your primary role is to analyze user's project description and interact with the user to gather all necessary details for their project idea. You are the initial point of contact and must ensure that the project idea and description is fully understood before it moves to the research phase.\n",
    "These are the messages that have been exchanged so far from the user asking for the report:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Assess whether you need to ask a clarifying question, or if the user has already provided enough information for you to start research.\n",
    "IMPORTANT: If you can see in the messages history that you have already asked a clarifying question, you almost always do not need to ask another one. Only ask another question if ABSOLUTELY NECESSARY.\n",
    "\n",
    "If there are acronyms, abbreviations, or unknown terms, ask the user to clarify.\n",
    "If you need to ask a question, follow these guidelines:\n",
    "- Be concise while gathering all necessary information\n",
    "- Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner.\n",
    "- Use bullet points or numbered lists if appropriate for clarity. Make sure that this uses markdown formatting and will be rendered correctly if the string output is passed to a markdown renderer.\n",
    "- Don't ask for unnecessary information, or information that the user has already provided. If you can see that the user has already provided the information, do not ask for it again.\n",
    "\n",
    "Respond in valid JSON format with these exact keys:\n",
    "\"need_clarification\": boolean,\n",
    "\"question\": \"<question to ask the user to clarify the report scope>\",\n",
    "\"verification\": \"<verification message that we will start research>\"\n",
    "\n",
    "If you need to ask a clarifying question, return:\n",
    "\"need_clarification\": true,\n",
    "\"question\": \"<your clarifying question>\",\n",
    "\"verification\": \"\"\n",
    "\n",
    "If you do not need to ask a clarifying question, return:\n",
    "\"need_clarification\": false,\n",
    "\"question\": \"\",\n",
    "\"verification\": \"<acknowledgement message that you will now start research based on the provided information>\"\n",
    "\n",
    "For the verification message when no clarification is needed:\n",
    "- Acknowledge that you have sufficient information to proceed\n",
    "- Briefly summarize the key aspects of what you understand from their request\n",
    "- Confirm that you will now begin the research process\n",
    "- Keep the message concise and professional\n",
    "\"\"\"\n",
    "transform_messages_into_research_topic_prompt = \"\"\"You will be given a set of messages that have been exchanged so far between yourself and the user.\n",
    "Your job is to translate these messages into a more detailed and concrete research question that will be used to guide the research.\n",
    "The messages that have been exchanged so far between yourself and the user are:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "You will return a single research question that will be used to guide the research.\n",
    "Guidelines:\n",
    "1. Maximize Specificity and Detail\n",
    "- Include all known user preferences and explicitly list key attributes or dimensions to consider.\n",
    "- It is important that all details from the user are included in the instructions.\n",
    "\n",
    "2. Fill in Unstated But Necessary Dimensions as Open-Ended\n",
    "- If certain attributes are essential for a meaningful output but the user has not provided them, explicitly state that they are open-ended or default to no specific constraint.\n",
    "\n",
    "3. Avoid Unwarranted Assumptions\n",
    "- If the user has not provided a particular detail, do not invent one.\n",
    "- Instead, state the lack of specification and guide the researcher to treat it as flexible or accept all possible options.\n",
    "\n",
    "4. Use the First Person\n",
    "- Phrase the request from the perspective of the user.\n",
    "\n",
    "5. Sources\n",
    "- If specific sources should be prioritized, specify them in the research question.\n",
    "- For academic or scientific queries, prefer linking directly to the original paper or official journal publication rather than survey papers or secondary summaries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5890bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    get_buffer_string,\n",
    ")\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "async def clarify_with_user(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"Clarify with user.\"\"\"\n",
    "    config = Configuration.from_runnable_config(config)\n",
    "    if not config.allow_clarification:\n",
    "        return Command(goto=\"write_research_brief\")\n",
    "    messages = state[\"messages\"]\n",
    "    model_config = {\n",
    "        \"model\": config.research_model,\n",
    "        \"max_tokens\": config.research_model_max_tokens,\n",
    "        # \"api_key\": config.research_model_api_key,\n",
    "    }\n",
    "    model = (\n",
    "        configurable_model.with_structured_output(ClarifyWithUser)\n",
    "        .with_retry(stop_after_attempt=config.max_structured_output_retries)\n",
    "        .with_config(model_config)\n",
    "    )\n",
    "    response = await model.ainvoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=clarify_with_user_instructions.format(\n",
    "                    messages=get_buffer_string(messages),\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    if response.need_clarification:\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\n",
    "                \"messages\": [*messages, AIMessage(content=response.question)],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return Command(\n",
    "        goto=\"write_research_brief\",\n",
    "        update={\n",
    "            \"messages\": [*messages, AIMessage(content=response.verification)],\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "async def write_research_brief(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"Create the research brief from previous conversations to prepare for research.\"\"\"\n",
    "    config = Configuration.from_runnable_config(config)\n",
    "    research_model_config = {\n",
    "        \"model\": config.research_model,\n",
    "        \"max_tokens\": config.research_model_max_tokens,\n",
    "        # \"api_key\": config.research_model_api_key,\n",
    "    }\n",
    "    research_model = (\n",
    "        configurable_model.with_structured_output(ResearchQuestion)\n",
    "        .with_retry(stop_after_attempt=config.max_structured_output_retries)\n",
    "        .with_config(research_model_config)\n",
    "    )\n",
    "    response: ResearchQuestion = await research_model.ainvoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=transform_messages_into_research_topic_prompt.format(\n",
    "                    messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    return Command(\n",
    "        goto=END,\n",
    "        update={\n",
    "            \"research_brief\": response.research_brief,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6ce2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x723edf959940>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarify_graph = StateGraph(\n",
    "    AgentState,\n",
    "    input_schema=AgentInputState,\n",
    "    context_schema=Configuration,\n",
    ")\n",
    "\n",
    "clarify_graph.add_node(\"clarify_with_user\", clarify_with_user)\n",
    "clarify_graph.add_node(\"write_research_brief\", write_research_brief)\n",
    "\n",
    "clarify_graph.add_edge(START, \"clarify_with_user\")\n",
    "clarify_graph.add_edge(\"clarify_with_user\", \"write_research_brief\")\n",
    "clarify_graph.add_edge(\"write_research_brief\", END)\n",
    "\n",
    "clarify_workflow = clarify_graph.compile(name=\"Clarify with User\")\n",
    "clarify_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654bbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_IDEA = \"create plan to develop agentic AI note taking app using langgraph for my personal use (personal project for fun and learning) and i also want to show off my skills to my potential interviewer to get hired. it should do following \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "739cd63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7. I am also thinking to use multi agent system one agent for image to text conversion and 2nd agent for text to notion or formatting (markdown)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"1. take pictures of hand-written notes \"\n",
    "\"2. it will automatically format the hand-written notes (it might contains equations and block diagrams) \"\n",
    "\"3. find proper section (if section found then create sub page or create a new page) in my notion \"\n",
    "\"4. add this notes with proper format\"\n",
    "\"5. i want to use LangGraph's pre-build UI for interaction from PC\"\n",
    "\"6. for MVP (which can convert image to text and format it properly) in 2 weeks\"\n",
    "\"7. I am also thinking to use multi agent system one agent for image to text conversion and 2nd agent for text to notion or formatting (markdown)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0edf6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_config = {\"configurable\": Configuration().model_dump()}\n",
    "\n",
    "# response = await clarify_workflow.ainvoke(\n",
    "#     {\"messages\": [HumanMessage(content=\"hello\")], \"config\": thread_config}\n",
    "# )\n",
    "# response\n",
    "response = await clarify_workflow.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=PROJECT_IDEA)], \"config\": thread_config},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4876bfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='create plan to develop agentic AI note taking app using langgraph for my personal use (personal project for fun and learning) and i also want to show off my skills to my potential interviewer to get hired. it should do following ', additional_kwargs={}, response_metadata={}, id='9eb4048b-0965-451b-9084-cc2384724640'),\n",
       "  AIMessage(content='Could you please specify which features you expect in your agentic AI note-taking app using LangGraph? For example, how should it manage notes, interact with users, or integrate with other services?', additional_kwargs={}, response_metadata={}, id='32e4f432-3da1-443e-9c5c-c180de8a4f76')],\n",
       " 'supervisor_message': [],\n",
       " 'research_brief': 'For my personal project aimed at developing an agentic AI note-taking application using LangGraph for personal use and potential professional showcase, I aim to define a detailed plan that includes specific features such as intelligent note management, user interaction, and service integration. How can I design an AI-driven note-taking app that effectively manages notes through advanced graph-based data structures (LangGraph), provides a seamless and interactive user experience, and integrates with other productivity tools? Additionally, what are the key technical considerations and best practices for developing such an application to ensure it meets my requirements for showcasing my skills to potential interviewers?'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ca4e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No synchronous function provided to \"clarify_with_user\".\nEither initialize with a synchronous function or invoke via the async API (ainvoke, astream, etc.)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m thread_config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: Configuration().model_dump()}\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# response = await clarify_workflow.ainvoke(\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#     {\"messages\": [HumanMessage(content=\"hello\")], \"config\": thread_config}\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# response\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclarify_workflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROJECT_IDEA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# obj = ClarifyWithUser.model_validate(msg.content)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# print(obj.question)\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# print(msg.content, end=\"\", flush=True)\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# print(metadata[\"langgraph_node\"], end=\"\", flush=True)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/practice-projects/Agents/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2651\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2649\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2650\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2651\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2661\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/practice-projects/Agents/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/practice-projects/Agents/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/practice-projects/Agents/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/practice-projects/Agents/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/practice-projects/Agents/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:646\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    644\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    645\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    648\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/practice-projects/Agents/project-planning-genie/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:323\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any\n\u001b[32m    321\u001b[39m ) -> Any:\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    324\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNo synchronous function provided to \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    325\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEither initialize with a synchronous function or invoke\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    326\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m via the async API (ainvoke, astream, etc.)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    327\u001b[39m         )\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    329\u001b[39m         config = ensure_config()\n",
      "\u001b[31mTypeError\u001b[39m: No synchronous function provided to \"clarify_with_user\".\nEither initialize with a synchronous function or invoke via the async API (ainvoke, astream, etc.)",
      "During task with name 'clarify_with_user' and id 'ed0b926b-9585-ff4e-4a4d-b1f74d5cad19'"
     ]
    }
   ],
   "source": [
    "thread_config = {\"configurable\": Configuration().model_dump()}\n",
    "\n",
    "# response = await clarify_workflow.ainvoke(\n",
    "#     {\"messages\": [HumanMessage(content=\"hello\")], \"config\": thread_config}\n",
    "# )\n",
    "# response\n",
    "async for msg, metadata in clarify_workflow.astream(\n",
    "    {\"messages\": [HumanMessage(content=PROJECT_IDEA)], \"config\": thread_config},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(msg)\n",
    "    # obj = ClarifyWithUser.model_validate(msg.content)\n",
    "    # print(obj.question)\n",
    "    # print(msg.content, end=\"\", flush=True)\n",
    "    # print(metadata[\"langgraph_node\"], end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572ceba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-planning-genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
